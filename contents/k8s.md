# 10-Day Intermediate Kubernetes and Harness CI/CD Learning Agenda

Below is a 10-day self-learning agenda for Kubernetes (K8s) and Harness Continuous Delivery/Integration, designed for intermediate learners with 8-hour daily sessions. Days 1–5 cover Kubernetes fundamentals using Kind (Kubernetes in Docker), Day 6 compares Amazon EKS to Kubernetes (no labs due to EKS costs), and Days 7–10 focus on Harness for CI/CD, integrating with Kubernetes and Helm charts. The agenda assumes basic Docker knowledge and includes hands-on labs with specific examples (e.g., Nginx, WordPress, Redis) for all days except Day 6.

| **Day** | **Key Learning Objectives** | **Labs/Activities (8 Hours)** | **Expected Output** |
|---------|-----------------------------|------------------------------|---------------------|
| **Day 1: Cluster Setup & Pods** | - Understand K8s architecture: nodes, pods, control plane.<br>- Set up a local cluster with Kind.<br>- Learn Pods and basic `kubectl` commands.<br>- Explore namespaces. | - Install Kind (`kind version`).<br>- Create a Kind cluster (`kind create cluster --name my-cluster`).<br>- Verify cluster with `kubectl get nodes`.<br>- Create a namespace `my-app`.<br>- Deploy a single Nginx Pod using YAML (image: `nginx:1.14.2`). Example YAML: `pod.yaml` with `apiVersion: v1`, `kind: Pod`, `spec.containers.image: nginx`.<br>- Use `kubectl apply -f pod.yaml -n my-app` and `kubectl get pods -n my-app`. | - Kind cluster running, `kubectl get nodes` shows at least 1 node.<br>- Namespace `my-app` created.<br>- Nginx Pod running, `kubectl get pods -n my-app` shows `nginx-pod` in `Running` state.<br>- `kubectl describe pod` shows pod details. |
| **Day 2: Deployments & Services** | - Learn Deployments for managing replicated apps.<br>- Understand Services (ClusterIP, NodePort).<br>- Explore rolling updates.<br>- Basic networking in K8s. | - Create a Deployment for a sample app (e.g., `httpd:2.4` image, 3 replicas). Example YAML: `deployment.yaml` with `kind: Deployment`, `spec.replicas: 3`.<br>- Expose Deployment with a ClusterIP Service. Example: `service.yaml` with `type: ClusterIP`, `port: 80`.<br>- Expose the same Deployment with a NodePort Service and access via `kubectl port-forward svc/<service-name> 8080:80` (e.g., `curl localhost:8080`).<br>- Update Deployment image to `httpd:2.4.57` and monitor with `kubectl rollout status`. | - Deployment running with 3 `httpd` pods, `kubectl get pods` shows 3 pods.<br>- ClusterIP and NodePort Services created, `kubectl get services` lists them.<br>- `curl localhost:8080` shows Apache welcome page via port-forward.<br>- Rolling update completes without downtime, `kubectl rollout status` confirms. |
| **Day 3: Helm Charts & Config Management** | - Understand Helm and Helm charts for app packaging.<br>- Learn ConfigMaps and Secrets for configuration.<br>- Manage app settings dynamically. | - Install Helm (`helm version`).<br>- Deploy a WordPress chart using `helm install my-wp bitnami/wordpress --namespace my-app`.<br>- Create a ConfigMap for app settings (e.g., `APP_ENV=production`). Example: `configmap.yaml` with `kind: ConfigMap`, `data.APP_ENV`.<br>- Create a Secret for a dummy DB password. Example: `secret.yaml` with `kind: Secret`, `data.password` (base64-encoded).<br>- Update the Day 2 Deployment to use ConfigMap and Secret via `env` in YAML.<br>- Verify with `kubectl exec` into a pod. | - Helm installed, WordPress deployed via Helm chart, accessible via `kubectl port-forward`.<br>- ConfigMap and Secret applied, `kubectl get configmaps,secrets -n my-app` lists them.<br>- `kubectl exec` into Day 2 pod confirms `APP_ENV` and password are set.<br>- `helm list -n my-app` shows WordPress release. |
| **Day 4: Storage & Ingress** | - Learn Persistent Volumes (PVC) and StorageClasses.<br>- Understand Ingress for HTTP routing.<br>- Set up an Ingress Controller. | - Create a PVC for a Redis pod (image: `redis:6.2`). Example: `pvc.yaml` with `kind: PersistentVolumeClaim`, `spec.storageClassName: standard`.<br>- Deploy Redis with PVC and test data persistence by saving a key (`redis-cli set mykey hello`).<br>- Install Nginx Ingress Controller via Helm: `helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx`.<br>- Create an Ingress rule to route `/app` to the Day 2 Service. Example: `ingress.yaml` with `kind: Ingress`, `spec.rules.path: /app`.<br>- Test with `kubectl port-forward svc/ingress-nginx-controller 8080:80` and `curl localhost:8080/app`. | - Redis pod running with PVC, `kubectl get pvc` shows bound claim.<br>- Redis data persists after pod restart (`redis-cli get mykey` returns `hello`).<br>- Ingress Controller running, `kubectl get pods -n ingress-nginx` shows pods.<br>- Ingress routes `/app` to Day 2 Service, `curl localhost:8080/app` returns Apache page. |
| **Day 5: Scaling & Troubleshooting** | - Understand Horizontal Pod Autoscaling (HPA).<br>- Learn basic troubleshooting with logs and probes.<br>- Explore K8s events and debugging. | - Set up HPA for Day 2 Deployment (target CPU: 50%). Example: `hpa.yaml` with `kind: HorizontalPodAutoscaler`, `spec.metrics.resource.name: cpu`.<br>- Simulate load using a script (e.g., `while true; do curl <nodeport-url>; done`) and observe scaling.<br>- Add liveness probe to Day 2 Deployment (e.g., `livenessProbe.httpGet.path: /`).<br>- Intentionally break a pod (e.g., wrong image tag) and troubleshoot using `kubectl logs`, `kubectl describe pod`, and `kubectl events`.<br>- Explore a sample Git repo for K8s manifests: `https://github.com/kubernetes/examples` (e.g., deploy guestbook app). | - HPA scales pods up/down, `kubectl get hpa` shows current/desired replicas.<br>- Liveness probe restarts pod on failure, `kubectl describe pod` shows probe events.<br>- Troubleshooting identifies bad image tag, fixed pod runs.<br>- Guestbook app deployed from Git repo, accessible via Service.<br>- `kubectl events` shows cluster activity. |
| **Day 6: EKS vs. Kubernetes Comparison** | - Understand Amazon EKS and its differences from vanilla Kubernetes.<br>- Explore EKS-specific features (e.g., managed control plane, integration with AWS services).<br>- Analyze trade-offs: cost, scalability, and management.<br>- Review EKS architecture and use cases. | - Study EKS architecture using AWS documentation: control plane, worker nodes, and VPC integration.<br>- Compare EKS to vanilla K8s (e.g., Kind): managed vs. self-hosted control plane, auto-scaling, IAM integration, and add-ons like AWS Load Balancer Controller.<br>- Review EKS pricing model (e.g., $0.10/hour per cluster, plus EC2 costs) and cost-saving strategies (e.g., Spot Instances).<br>- Explore EKS use cases (e.g., production workloads, hybrid cloud) vs. Kind (local dev/testing).<br>- Create a comparison table in a document (e.g., `eks-vs-k8s.md`) covering setup, scalability, security, and maintenance.<br>- Watch AWS EKS webinars or tutorials (e.g., AWS YouTube channel) for visual insights. | - `eks-vs-k8s.md` document with a detailed comparison table (e.g., EKS managed control plane vs. Kind self-managed, EKS IAM vs. K8s RBAC).<br>- Clear understanding of EKS benefits (e.g., AWS integrations) and trade-offs (e.g., cost, complexity).<br>- Notes summarizing EKS use cases and pricing considerations.<br>- Familiarity with EKS architecture from AWS docs and videos. |
| **Day 7: Introduction to Harness & Delegate Setup** | - Understand Harness Continuous Integration (CI) and Continuous Delivery (CD) concepts.<br>- Set up a Harness account and CI pipeline.<br>- Install and configure a Harness Delegate.<br>- Build and push Docker images. | - Sign up for a Harness account at `app.harness.io`.<br>- Create a Harness project and select the CI module.<br>- Install a Harness Delegate in the Kind cluster: Download the Delegate YAML from Harness UI (`Setup > Account Settings > Delegates > New Delegate`), apply with `kubectl apply -f delegate.yaml -n harness-delegate`.<br>- Verify Delegate status in Harness UI (shows as connected).<br>- Set up a GitHub connector for a sample repo (e.g., `https://github.com/harness-community/harnesscd-example-apps`).<br>- Configure a Docker connector for Docker Hub.<br>- Create a CI pipeline to build and push a Docker image (e.g., `nginx:1.14.2` with a custom tag). Example: Use `Dockerfile` from the repo to build and push to `yourusername/nginx:demo`.<br>- Run the pipeline and verify the image in Docker Hub. | - Harness project created with CI module enabled.<br>- Delegate pod running in `harness-delegate` namespace, `kubectl get pods -n harness-delegate` shows pod.<br>- Harness UI confirms Delegate connection.<br>- GitHub and Docker connectors configured.<br>- CI pipeline builds and pushes `yourusername/nginx:demo` to Docker Hub.<br>- Pipeline logs show successful build and push.<br>- Docker Hub shows the new image tag. |
| **Day 8: Harness CD with Helm Charts** | - Understand Harness CD for Kubernetes.<br>- Learn to deploy Helm charts using Harness.<br>- Explore Harness services and environments.<br>- Understand automated rollbacks. | - In the Harness project, enable the CD module.<br>- Create a Harness service for the Bitnami Nginx Helm chart (`bitnami/nginx`, version `5.1.4`). Example: Use `https://charts.bitnami.com/bitnami` as the Helm repo URL.<br>- Set up a Harness environment (`dev`) with a Kind cluster infrastructure (use kubeconfig from Kind).<br>- Create a CD pipeline to deploy the Nginx Helm chart to the `my-app` namespace.<br>- Simulate a failed deployment (e.g., invalid chart version) and observe Harness rollback.<br>- Verify deployment with `kubectl get pods -n my-app`. | - Harness service and environment configured for Nginx Helm chart.<br>- CD pipeline deploys Nginx chart to Kind cluster.<br>- Failed deployment triggers automatic rollback, pipeline logs confirm.<br>- `kubectl get pods -n my-app` shows Nginx pods running.<br>- `helm list -n my-app` shows the deployed release. |
| **Day 9: Harness Pipeline Automation** | - Learn to automate Harness pipelines with triggers.<br>- Explore Harness secrets management.<br>- Understand pipeline variables and inputs. | - In the Harness project, create a trigger for the Day 7 CI pipeline using a GitHub webhook (e.g., trigger on push to `main` in `harnesscd-example-apps` repo).<br>- Create a Harness secret for a GitHub Personal Access Token (PAT) (`harness_gitpat`) to authenticate the webhook.<br>- Add a pipeline variable to the Day 8 CD pipeline (e.g., `replica_count` set to 3) to customize the Nginx chart’s `replicaCount`.<br>- Run the CI pipeline via a manual trigger and the CD pipeline with the variable.<br>- Test the webhook by pushing a small change to the repo (e.g., update `README.md`).<br>- Verify with `kubectl get pods -n my-app` for the CD pipeline result. | - Webhook trigger configured, pipeline runs on repo push, logs confirm execution.<br>- Secret `harness_gitpat` created and used in trigger.<br>- CD pipeline uses `replica_count` variable, `kubectl get pods -n my-app` shows 3 Nginx pods.<br>- CI pipeline runs manually and via webhook, Docker Hub shows updated image.<br>- Pipeline logs show variable application and successful runs. |
| **Day 10: Harness Troubleshooting & Review** | - Understand Harness pipeline debugging and rollback.<br>- Explore Kubernetes diff previews in Harness.<br>- Review all 10 days’ concepts.<br>- Consolidate learnings. | - Simulate a failure in the Day 8 CD pipeline (e.g., invalid `values.yaml` for Nginx chart) and debug using Harness pipeline logs.<br>- Trigger a manual rollback in Harness and verify with `kubectl get pods -n my-app`.<br>- Enable Kubernetes diff preview in the Day 8 pipeline to compare manifests before deployment.<br>- Re-run pipelines from Days 7–9 and troubleshoot any issues using Harness UI and `kubectl`.<br>- Review all YAMLs, Helm charts, and pipelines from the 10 days.<br>- Create a summary file (e.g., `k8s-harness-notes.md`) documenting key learnings from Kubernetes, EKS comparison, and Harness. | - Pipeline failure debugged, logs identify invalid `values.yaml`, rollback successful.<br>- Diff preview shows manifest changes before deployment.<br>- All pipelines from Days 7–9 run successfully, `kubectl` confirms deployments.<br>- `k8s-harness-notes.md` created with summaries of Kubernetes (Pods, Services, Helm, etc.), EKS trade-offs, and Harness CI/CD workflows.<br>- Clear understanding of debugging and rollback processes in Harness. |

## Notes
- **Prerequisites**: Install Kind, kubectl, Helm, Docker, and a code editor. Ensure Docker has 4GB RAM and 2 CPUs. Sign up for a Harness account (`app.harness.io`). Basic YAML, Docker, and Git knowledge required. No AWS account needed for Day 6 (EKS study is theoretical).
- **Time Allocation**: ~3 hours theory (K8s/Harness/AWS docs, Helm tutorials), ~4 hours labs (except Day 6), ~1 hour troubleshooting/review per day. Day 6: ~6 hours study, ~2 hours documentation.
- **Resources**: Use [Kubernetes Docs](https://kubernetes.io/docs/), [Helm Docs](https://helm.sh/docs/), [Harness Docs](https://developer.harness.io/), [AWS EKS Docs](https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html), and GitHub repos [kubernetes/examples](https://github.com/kubernetes/examples) and [harnesscd-example-apps](https://github.com/harness-community/harnesscd-example-apps). For Nginx, use `nginx:1.14.2`; for WordPress, use Bitnami’s Helm chart.
- **Environment**: Kind is used for lightweight clusters. Use `kubectl port-forward` for service/Ingress access. Harness pipelines require a Harness Delegate installed in the Kind cluster (Day 7).
- **Harness Setup**: Follow Harness Quickstart for Kubernetes and Helm deployments. Use free-tier Harness for labs, but note potential usage limits.
- **Day 6 Notes**: EKS study uses free resources (AWS docs, webinars). No labs to avoid costs. Focus on understanding EKS vs. Kind for production vs. local use cases.
- **Simplifications**: Days 9–10 focus on pipeline automation, debugging, and review, excluding GitOps and Continuous Verification for simplicity. Harness labs build on Kubernetes concepts from Days 1–5.