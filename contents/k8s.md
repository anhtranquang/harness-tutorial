| **Day** | **Key Learning Objectives** | **Labs/Activities (8 Hours)** | **Expected Output** |
|---------|-----------------------------|------------------------------|---------------------|
| **Day 1: Cluster Setup & Core Concepts** (8 hours) | - Master K8s architecture: control plane, worker nodes, etcd, API server, scheduler, kubelet.<br>- Set up a multi-node cluster.<br>- Explore RBAC and namespaces.<br>- Understand pod scheduling and taints/taints. | - Install Minikube or Kind with multi-node support (e.g., `minikube start --nodes 3`).<br>- Configure `kubectl` with a kubeconfig file.<br>- Create namespaces (e.g., `dev`, `prod`).<br>- Set up a simple RBAC policy (e.g., role to allow pod listing in `dev`).<br>- Apply taints to a node and deploy a pod with tolerations.<br>- Use `kubectl describe node` and `kubectl get pods -A`. | - Multi-node cluster running (3 nodes visible via `kubectl get nodes`).<br>- Namespace and RBAC YAML files applied.<br>- Pod scheduled on tainted node (verified via `kubectl get pods -o wide`).<br>- Clear understanding of cluster components via `kubectl` outputs. |
| **Day 2: Advanced Workloads & Controllers** (8 hours) | - Deep dive into Pods, Deployments, StatefulSets, and DaemonSets.<br>- Understand ReplicaSets and rolling updates.<br>- Explore Jobs and CronJobs for batch processing.<br>- Learn pod affinity/anti-affinity. | - Deploy a StatefulSet for a MongoDB replica set (3 replicas).<br>- Create a DaemonSet to run a logging agent (e.g., Fluentd) on all nodes.<br>- Set up a CronJob to run a backup script every 15 minutes.<br>- Configure pod affinity to co-locate an app with its cache (e.g., Redis).<br>- Perform a rolling update on a Deployment (`kubectl set image`).<br>- Monitor with `kubectl rollout status`. | - StatefulSet running with 3 MongoDB pods and persistent storage.<br>- DaemonSet logs collected (verified via `kubectl logs`).<br>- CronJob executed at least once (check `kubectl get jobs`).<br>- Affinity rules applied, pods co-located (check `kubectl get pods -o wide`).<br>- Successful rolling update with no downtime. |
| **Day 3: Networking & Ingress** (8 hours) | - Master K8s Services: ClusterIP, NodePort, LoadBalancer, ExternalName.<br>- Understand Ingress and Ingress Controllers.<br>- Explore DNS and CoreDNS in K8s.<br>- Learn network policies for pod isolation. | - Deploy an Ingress Controller (e.g., Nginx via Helm).<br>- Create an Ingress resource to route traffic to two services (e.g., `/app1` and `/app2`).<br>- Set up a NetworkPolicy to restrict traffic to a specific namespace.<br>- Test service discovery using `kubectl exec` to resolve DNS.<br>- Expose a service via LoadBalancer and test with `minikube tunnel`.<br>- Use `kubectl port-forward` for debugging. | - Ingress routes traffic correctly (e.g., `curl http://<minikube-ip>/app1` works).<br>- NetworkPolicy restricts traffic (verified by failed `curl` to restricted pod).<br>- LoadBalancer service accessible externally.<br>- DNS resolution confirmed within a pod.<br>- Helm chart for Ingress Controller installed. |
| **Day 4: Storage, Config Management & Observability** (8 hours) | - Deep dive into Persistent Volumes (PV), Persistent Volume Claims (PVC), and StorageClasses.<br>- Master ConfigMaps and Secrets for app configuration.<br>- Set up basic monitoring with Prometheus and Grafana.<br>- Explore K8s events and logging. | - Create a dynamic StorageClass with a provisioner (e.g., local-path).<br>- Deploy a MySQL pod with a PVC and test data persistence.<br>- Create a ConfigMap for app settings and a Secret for DB credentials.<br>- Install Prometheus and Grafana using Helm.<br>- Configure a dashboard to monitor pod CPU/memory usage.<br>- Analyze logs and events with `kubectl logs` and `kubectl describe`. | - MySQL pod retains data after restart (verified via `kubectl exec`).<br>- ConfigMap and Secret injected into pod (check env vars).<br>- Prometheus scrapes metrics, Grafana dashboard shows pod metrics.<br>- Logs and events retrieved for a specific pod.<br>- StorageClass and PVC YAML files applied. |
| **Day 5: Scaling, CI/CD & Troubleshooting** (8 hours) | - Understand Horizontal Pod Autoscaling (HPA) and Cluster Autoscaler.<br>- Learn K8s in CI/CD pipelines (e.g., GitOps with ArgoCD).<br>- Master troubleshooting techniques (logs, events, probes).<br>- Explore custom resources and operators. | - Set up HPA for a Deployment based on CPU usage (use `kubectl autoscale`).<br>- Simulate load with a tool like `wrk` to trigger scaling.<br>- Install ArgoCD and deploy an app via GitOps (use a GitHub repo).<br>- Create a liveness/readiness probe for a pod and test failure scenarios.<br>- Deploy a simple custom operator (e.g., sample from Operator SDK).<br>- Troubleshoot a failed pod using `kubectl describe`, `kubectl logs`, and `kubectl debug`. | - HPA scales pods up/down based on load (check `kubectl get hpa`).<br>- ArgoCD syncs app from Git repo (visible in ArgoCD UI).<br>- Probes correctly handle pod health (verified by pod restarts).<br>- Custom operator deployed and functional (e.g., creates resources).<br>- Troubleshooting identifies and resolves a pod failure (e.g., misconfigured image). |